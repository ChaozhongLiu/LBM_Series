{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY6rneHZFidE",
        "outputId": "9b742af5-18f0-43f0-a733-0bf5f0d45ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformers tokenizers Implementation"
      ],
      "metadata": {
        "id": "sGWsLDXYDgIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import json"
      ],
      "metadata": {
        "id": "mZi_SsfSzu-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Prepare toy sequence data and Iterator\n",
        "genome_sequence = 100*\"AAAAAAACACGCTAATTGCCCGCTTAGATCCCGATTGCTGCTCGTGCTGCTGCTATATATATATATACCCCGTTACTTGAACTGGCA\"\n",
        "def batch_iterator(genome_sequence, batch_size=75):\n",
        "    for i in range(0, len(genome_sequence), batch_size):\n",
        "        yield [genome_sequence[i: i + batch_size]]"
      ],
      "metadata": {
        "id": "ZcI4XLO4E59a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# Step 2: Set pre-tokenizer\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# Step 3: Train the tokenizer\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=4096,\n",
        "    special_tokens=[\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        "    )\n",
        "tokenizer.train_from_iterator(batch_iterator(genome_sequence), trainer=trainer)\n",
        "\n",
        "# Step 4: Set the post-processor\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Step 5: Save the trained tokenizer and config\n",
        "tokenizer.save(\"tokenizer.json\")\n",
        "\n",
        "config = {\n",
        "    \"unk_token\": \"[UNK]\",\n",
        "    \"cls_token\": \"[CLS]\",\n",
        "    \"sep_token\": \"[SEP]\",\n",
        "    \"pad_token\": \"[PAD]\",\n",
        "    \"mask_token\": \"[MASK]\"\n",
        "}\n",
        "with open(\"tokenizer_config.json\", \"w\") as f:\n",
        "    json.dump(config, f)"
      ],
      "metadata": {
        "id": "Tp8vHU33zu8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer configuration\n",
        "with open(\"tokenizer_config.json\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Load the tokenizer using PreTrainedTokenizerFast\n",
        "toy_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\", **config)\n",
        "\n",
        "# Example usage of the fast tokenizer\n",
        "encoded = toy_tokenizer(\"ACTGACTGACTG\")\n",
        "print(encoded)\n",
        "decoded = toy_tokenizer.decode(encoded[\"input_ids\"])\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZpOX2D7zu56",
        "outputId": "610a9a5f-32cf-4e0e-9e24-fccd9a4524ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [1, 5, 25, 5, 25, 5, 25, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "[CLS] A CTG A CTG A CTG [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's check the toy Tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "pM1wMgUiFTpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQvREeJTFSUh",
        "outputId": "edb81088-92f7-44c3-be1c-1b5ed1e9d1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='', vocab_size=153, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's check the real DNABERT-2 Tokenizer\n",
        "It's the same (except vacab size) with our toy tokenzier"
      ],
      "metadata": {
        "id": "QTITjty-FaSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "real_tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
        "real_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-oPvUy2zu1B",
        "outputId": "1e9679b3-6b87-40b1-9b0a-6e6ef8a311e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizerFast(name_or_path='zhihan1996/DNABERT-2-117M', vocab_size=4096, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9dDsU-XFC5G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SentencePiece Style"
      ],
      "metadata": {
        "id": "fMgMWy5HDXGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "fQQeWg10Fz_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write a toy.txt file with some random text\n",
        "with open(\"toy_genome.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(\"AACGCTTGCTAGCTAGCAATTGCCCGCTTAGATCCCGATTGCTGCTCGTGCTGCTGCTATATATATATATACCCCGTTACTTGAACTGGCA\")"
      ],
      "metadata": {
        "id": "W9NQNYOGFz88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "\n",
        "options = dict(\n",
        "  # Input-related\n",
        "  input=\"toy_genome.txt\",               # Training dataset file\n",
        "  input_format=\"text\",                  # Format of training dataset\n",
        "\n",
        "  # Output-related\n",
        "  model_prefix=\"dnabert2\",              # Output filename prefix\n",
        "\n",
        "  # Algorithm\n",
        "  model_type=\"bpe\",                     # Use BPE Algorithm\n",
        "  vocab_size=50,                        # Vocabulary size, DNABERT-2 size = 4096\n",
        "\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\",   # Turn off normalization\n",
        "\n",
        "  # rare word treatment\n",
        "  byte_fallback=False,                  # Turn off byte-level fallback\n",
        "\n",
        "  # merge rules\n",
        "  max_sentencepiece_length=16,          # Set of the max length of a token\n",
        "  add_dummy_prefix=False,               # Don't add '_' in the begining of seq\n",
        "\n",
        "  # special tokens\n",
        "  unk_piece=\"[UNK]\",                    # UNK token\n",
        "  bos_piece=\"[CLS]\",                    # BOS token\n",
        "  eos_piece=\"[SEP]\",                    # EOS token\n",
        "  pad_piece=\"[PAD]\",                    # PAD token\n",
        "  unk_id=0,\n",
        "  bos_id=1,\n",
        "  eos_id=2,\n",
        "  pad_id=3,\n",
        "  user_defined_symbols='[MASK]',        # Special token needed for DNABERT-2\n",
        "\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)\n"
      ],
      "metadata": {
        "id": "tjhXwDlLFz6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('dnabert2.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
        "#vocab"
      ],
      "metadata": {
        "id": "Gnyu80RnFz4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"AATATCGATTC\")\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3WE0Y4GFz1r",
        "outputId": "8b1a3f89-491c-4677-b0f9-dc2cce56b2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 6, 26, 18, 46, 47]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([sp.id_to_piece(idx) for idx in ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmVyReDJFzyj",
        "outputId": "e4b3561a-32b2-41ef-a0f5-7e1498fc2b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AA', 'TA', 'TCG', 'AT', 'T', 'C']\n"
          ]
        }
      ]
    }
  ]
}